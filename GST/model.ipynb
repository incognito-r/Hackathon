{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Preprocessing Function\n",
    "\n",
    "def preprocess_input_data(df, treat_na=True, treat_outliers=True, train=True,\n",
    "                          lower_quantile=0.01, upper_quantile=0.99):\n",
    "\n",
    "    def drop_missing_columns(df, keep_threshold=0.3, drop_threshold=0.5):\n",
    "        missing_percentage = df.isnull().mean()\n",
    "\n",
    "        columns_to_drop = missing_percentage[missing_percentage >\n",
    "                                             drop_threshold].index.tolist()\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        for col in missing_percentage.index:\n",
    "            if keep_threshold < missing_percentage[col] <= drop_threshold:\n",
    "                user_input = input(f\"Column '{col}' has {missing_percentage[col]:.2%} missing values. \"\n",
    "                                   f\"Do you want to keep (K), drop (D), or treat (T) this column? \").strip().lower()\n",
    "                if user_input == 'd':\n",
    "                    df = df.drop(columns=[col])\n",
    "                elif user_input == 't':\n",
    "                    if df[col].dtype in ['float64', 'int64']:\n",
    "                        df[col] = df[col].fillna(df[col].median())\n",
    "                    else:\n",
    "                        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Step 1: Drop columns with excessive missing values\n",
    "    df = drop_missing_columns(df)\n",
    "\n",
    "    # Step 2: Identify numeric and categorical columns\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    # Step 3: Normalize string categorical data\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].str.lower()  # Convert to lowercase for consistency\n",
    "\n",
    "    # Step 4: Handle missing values if specified\n",
    "    if treat_na:\n",
    "        df[num_cols] = df[num_cols].fillna(df[num_cols].median(\n",
    "            numeric_only=True))  # Fill numeric with median\n",
    "\n",
    "        for col in cat_cols:\n",
    "            if col in df.columns and not df[col].isnull().all():\n",
    "                # Fill categorical with mode\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Step 5: Identify and map binary columns\n",
    "    binary_columns = []\n",
    "\n",
    "    for col in num_cols:\n",
    "        if df[col].nunique() == 2:\n",
    "            binary_columns.append(col)\n",
    "            df[col] = df[col].map(lambda x: 1 if x ==\n",
    "                                  1 else 0)  # Map to 1 and 0\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if df[col].nunique() == 2:\n",
    "            binary_columns.append(col)\n",
    "            df[col] = df[col].map(lambda x: 1 if x ==\n",
    "                                  df[col].unique()[1] else 0)\n",
    "\n",
    "    # Step 6: Outlier treatment if specified\n",
    "    if treat_outliers:\n",
    "        def cap_outliers(series, lower_q, upper_q):\n",
    "            lower_bound = series.quantile(lower_q)\n",
    "            upper_bound = series.quantile(upper_q)\n",
    "            # Cap the outliers\n",
    "            return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "        for col in num_cols:\n",
    "            if col not in binary_columns:  # Skip binary columns\n",
    "                df[col] = cap_outliers(df[col], lower_quantile, upper_quantile)\n",
    "\n",
    "    # Step 7: Scaling numeric data\n",
    "    scaler = StandardScaler()  # Initialize the scaler\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])  # Scale numeric columns\n",
    "\n",
    "    # Return processed DataFrame, scaler, and binary column names\n",
    "    return df, scaler, binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Training Function\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import joblib\n",
    "\n",
    "def train_model(df_train_X, df_train_Y):\n",
    "\n",
    "    def save_model(model, scaler, model_path='./model/model.pkl', scaler_path='./model/scaler.pkl'):\n",
    "        model_dir = os.path.dirname(model_path)\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "            print(f\"Directory '{model_dir}' created.\")\n",
    "\n",
    "        joblib.dump(model, model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"Model saved to {model_path} \\nScaler saved to {scaler_path}\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    df_train_clean, scaler, _ = preprocess_input_data(\n",
    "        df_train_X, treat_na=True, treat_outliers=True, lower_quantile=0.05, upper_quantile=0.95\n",
    "    )\n",
    "\n",
    "    # Extract feature matrix and target labels\n",
    "    X_train = df_train_clean.iloc[:, 1:]  # Exclude ID column\n",
    "    # Ensure that 'target' column exists in df_train_Y\n",
    "    y_train = df_train_Y['target']\n",
    "\n",
    "    # Initialize and train logistic regression model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "\n",
    "        # Save the model and scaler\n",
    "        save_model(model, scaler)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training: {e}\")\n",
    "        raise\n",
    "        \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(input_df, model, scaler):\n",
    "\n",
    "    # Check if the input DataFrame is empty\n",
    "    if input_df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "\n",
    "    # Ensure the ID column is present\n",
    "    if 0 not in input_df.columns:\n",
    "        raise ValueError(\n",
    "            \"Input DataFrame must contain an ID column at index 0.\")\n",
    "\n",
    "    # Extract the ID column\n",
    "    IDs = input_df.iloc[:, 0]  # Assuming the first column is ID\n",
    "\n",
    "    # Preprocess the input data\n",
    "    input_clean, scaler, _ = preprocess_input_data(\n",
    "        input_df, treat_na=True, treat_outliers=True, train=False)\n",
    "\n",
    "    # Check if there are any features to predict\n",
    "    if input_clean.shape[1] < 2:\n",
    "        raise ValueError(\n",
    "            \"Input DataFrame must have at least one feature column for prediction.\")\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(\n",
    "        input_clean.iloc[:, 1:])  # Exclude the ID column\n",
    "\n",
    "    # Return a DataFrame with the ID and predicted values\n",
    "    return pd.DataFrame({\n",
    "        'ID': IDs,\n",
    "        'Predicted': predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = pd.read_csv(\"./train/Train_60/X_train_Data_Input.csv\")\n",
    "df_train_Y = pd.read_csv(\"./train/Train_60/Y_train_Data_Target.csv\")\n",
    "df_test_X = pd.read_csv('./test/Test_20/X_Test_Data_Input.csv')\n",
    "df_test_Y = pd.read_csv(\"./test/Test_20/Y_Test_Data_Target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns for df_train_X and df_test_X\n",
    "# df_train_X.columns = ['ID'] + [str(i) for i in range(df_train_X.shape[1] - 1)]\n",
    "# df_test_X.columns = ['ID'] + [str(i) for i in range(df_test_X.shape[1] - 1)]\n",
    "\n",
    "# Rename columns for df_train_X and df_test_X to integers\n",
    "df_train_X.columns = range(df_train_X.shape[1])  # 0, 1, 2, ...\n",
    "df_test_X.columns = range(df_test_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad1a67e4cbddc767a3456b0d94299b9e</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2495</td>\n",
       "      <td>3726.0</td>\n",
       "      <td>0.678139</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>0.434190</td>\n",
       "      <td>-0.015603</td>\n",
       "      <td>0.606265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7246d2f76ac0c217ec25e72ea5f014cb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2495</td>\n",
       "      <td>3454.0</td>\n",
       "      <td>0.452580</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>-0.007468</td>\n",
       "      <td>1.554998</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>0.329946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22ba388e7dd14c13342c49e75fc29dda</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2495</td>\n",
       "      <td>4543.0</td>\n",
       "      <td>-1.577453</td>\n",
       "      <td>-1.429540</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>-0.407939</td>\n",
       "      <td>-0.015607</td>\n",
       "      <td>-0.774979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59f9b981472d97342587fb3e6392aeb1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.407939</td>\n",
       "      <td>-0.015607</td>\n",
       "      <td>-0.774979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f6317cf7ecf126859804eddff279aead</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718</td>\n",
       "      <td>950.0</td>\n",
       "      <td>-2.028572</td>\n",
       "      <td>-1.855728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.407939</td>\n",
       "      <td>-0.015607</td>\n",
       "      <td>-0.774979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0    1     2       3         4         5   \\\n",
       "0  ad1a67e4cbddc767a3456b0d94299b9e  2.0  2495  3726.0  0.678139  0.701403   \n",
       "1  7246d2f76ac0c217ec25e72ea5f014cb  0.0  2495  3454.0  0.452580  0.701403   \n",
       "2  22ba388e7dd14c13342c49e75fc29dda  2.0  2495  4543.0 -1.577453 -1.429540   \n",
       "3  59f9b981472d97342587fb3e6392aeb1  0.0   211    59.0       NaN       NaN   \n",
       "4  f6317cf7ecf126859804eddff279aead  0.0   718   950.0 -2.028572 -1.855728   \n",
       "\n",
       "         6         7         8         9   10  11  12  13  14        15  \\\n",
       "0 -0.007468  0.434190 -0.015603  0.606265 NaN   0   0   0   0  0.001351   \n",
       "1 -0.007468  1.554998 -0.015574  0.329946 NaN   0   0   0   0  0.001351   \n",
       "2 -0.007469 -0.407939 -0.015607 -0.774979 NaN   1   1   1   1  0.001351   \n",
       "3       NaN -0.407939 -0.015607 -0.774979 NaN   0   0   0   0       NaN   \n",
       "4       NaN -0.407939 -0.015607 -0.774979 NaN   0   0   0   0       NaN   \n",
       "\n",
       "        16   17  18   19  20  21  22  \n",
       "0  0.00339  0.0   0  0.0   0   0   0  \n",
       "1  0.00339  0.0   0  0.0   0   0   0  \n",
       "2  0.00339  0.0   0  0.0   0   0   0  \n",
       "3  0.00339  0.0   0  1.0   0   0   0  \n",
       "4  0.00339  0.0   0  0.0   0   0   0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess training data\n",
    "# df_train_clean, scaler, binary_columns = preprocess_input_data(\n",
    "#     df_train_X, treat_na=True, treat_outliers=True, lower_quantile=0.05, upper_quantile=0.95\n",
    "# )\n",
    "\n",
    "# print(\"\\nCleaned-Normalised-Training Data:\")\n",
    "# df_train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess test data using same scaler\n",
    "# df_test_clean, _, _ = preprocess_input_data(\n",
    "#     df_test_X, treat_na=True, treat_outliers=True, lower_quantile=0.05, upper_quantile=0.95\n",
    "# )\n",
    "\n",
    "# print(\"\\nCleaned-Normalised-Test Data:\")\n",
    "# df_test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n",
      "Model saved to ./model/model.pkl \n",
      "Scaler saved to ./model/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, scaler = train_model(df_train_X, df_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07cf2025382f6325b316e128b1b90999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb972eb3a1f8d0d1a13f45e7c07d37d4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ee35e164b3ddc25a9f40243b81ad290d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28229ccd7bad7dd83324a4175a7e0531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2f94873da2c332d28f111742818e0fbb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  Predicted\n",
       "0  07cf2025382f6325b316e128b1b90999          0\n",
       "1  eb972eb3a1f8d0d1a13f45e7c07d37d4          0\n",
       "2  ee35e164b3ddc25a9f40243b81ad290d          0\n",
       "3  28229ccd7bad7dd83324a4175a7e0531          0\n",
       "4  2f94873da2c332d28f111742818e0fbb          0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predicted_df = predictor(df_test_X, model, scaler)\n",
    "predicted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted\n",
       "0    242756\n",
       "1     18956\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df['Predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07cf2025382f6325b316e128b1b90999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb972eb3a1f8d0d1a13f45e7c07d37d4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ee35e164b3ddc25a9f40243b81ad290d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28229ccd7bad7dd83324a4175a7e0531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2f94873da2c332d28f111742818e0fbb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  target\n",
       "0  07cf2025382f6325b316e128b1b90999       0\n",
       "1  eb972eb3a1f8d0d1a13f45e7c07d37d4       0\n",
       "2  ee35e164b3ddc25a9f40243b81ad290d       0\n",
       "3  28229ccd7bad7dd83324a4175a7e0531       0\n",
       "4  2f94873da2c332d28f111742818e0fbb       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    237034\n",
      "           1       0.85      0.66      0.74     24678\n",
      "\n",
      "    accuracy                           0.96    261712\n",
      "   macro avg       0.91      0.82      0.86    261712\n",
      "weighted avg       0.95      0.96      0.95    261712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report on test data\n",
    "report_test = metrics.classification_report(\n",
    "    y_true=df_test_Y['target'], y_pred=predicted_df['Predicted'])\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ** Input Validation**: Add checks to ensure that the input DataFrames(`df_train_Y` and `df_test_X`) have the correct structure and necessary columns.\n",
    "\n",
    "2. ** User Interaction in `drop_missing_columns`** : Consider making user prompts optional or implement a logging mechanism for smoother user experience, especially for batch processing.\n",
    "\n",
    "3. ** Error Logging**: Use the `logging` module instead of printing errors to facilitate easier debugging in larger applications.\n",
    "\n",
    "4. ** Model Persistence**: Save the trained model using libraries like `joblib` or `pickle` to reuse it without needing to retrain.\n",
    "\n",
    "5. ** Function Flexibility**: Parameterize additional aspects of preprocessing or model training to accommodate various datasets and use cases.\n",
    "\n",
    "6. ** Enhance Documentation**: Ensure all functions are well-documented with clear descriptions of parameters and return values to improve usability.\n",
    "\n",
    "7. ** Cross-Validation**: Implement cross-validation in the model training process to evaluate model performance more robustly.\n",
    "\n",
    "8. ** Hyperparameter Tuning**: Consider adding a mechanism for hyperparameter tuning(e.g., using GridSearchCV) to optimize model performance.\n",
    "\n",
    "9. ** Output Verification**: Include assertions or checks to verify the shape and content of outputs at various stages, ensuring consistency.\n",
    "\n",
    "10. ** Testing**: Develop unit tests for each function to verify their functionality and correctness, enhancing reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanFeatures(df):\n",
    "\n",
    "    rows, cols = df.shape[0], df.shape[1]\n",
    "    \n",
    "    new_col_names = [f\"c{x}\" for x in range(cols-1)]\n",
    "    df.columns = [df.columns[0]] + new_col_names\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanFeatures(df_train_X).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = cleanFeatures(df_train_X)\n",
    "df_train_X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
